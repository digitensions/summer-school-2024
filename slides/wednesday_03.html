<!DOCTYPE html>
<html>
  <head>
    <title>Digital Summer School 2024: WED03</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="../style.css">  </head>
  <body>
    <textarea id="source">




class: center, middle, titlepage
### WED03: *Extracting content metadata.*




---
class: contentpage
### **Content Metadata**

We have looked a bit at extracting technical metadata from media, but now we are going to look at something else - extracting metadata around the content of the media.

This is an area which is being revolutionised by Large Language Models aka Machine Learning aka AI (like ChatGPT).


---
class: contentpage
### **Content Metadata**


Prior to around 2022, there were many projects in this space, using a variety of different techniques, often stretching back decades.

Speech to text.    
OCR.    
Image analysis.    
Experimental/Other.

---
class: contentpage
### **Content Metadata**

Prior to around 2022, there were many projects in this space, using a variety of different techniques, often stretching back decades.

~~Speech to text.~~ now -> Whisper    
~~OCR.~~ now -> LLM        
~~Image analysis.~~ now -> LLM     
~~Experimental/Other.~~ now -> LLM   



---
class: contentpage
### **Machine Learning**


These systems all have in common the idea of an artificial neural network, an idea which has been around since the 1940s, but has only achieved recent widespread use due to computational power and innovative strategies.

An artificial neural network very roughly approximates the way our own brains work, in that they comprise many many many very simple "nodes", which develop to behave in certain ways, and complexity arises out of many many nodes communicating with many many other nodes.

---
class: contentpage
### **Machine Learning**

A traditional artificial neural network has three components, "inputs", "hidden layers", and "outputs".

- The inputs could be numbers, letters, pixels, audio samples, as long as the expressed as numbers.

- The hidden layers sit in-between. These are sequential layers of nodes which are all interconnected to one direction. Often these would start off as randomised noise, so will produce useless results, but if we have known input which result in expected outputs, we can start to "train" all the nodes in the hidden layer to start to behave in a certain way.

- The outputs could be as simple as a "yes" or "no" answer to the question, "is this a Spanish word?"

The power of artificial neural networks are that they can apply to almost any subject or discipline, the downside is that the hidden layers are not explicable - we get to a place where we cannot explain how a certain trained neural net (or "model") is producing a certain result.

Depending on source material, we can also see undesired biases replicated. 

[NNFS](https://nnfs.io/) is a wonderful resource which contains examples of building a neural network using just vanilla Python.

Of course, like cars, we don't have to understand fully how artifical neural networks work to use them!!

---
class: contentpage
### **Whisper**


With this context, we can now start to look at Whisper from OpenAI. 

Whisper is now so ubiquitous it has almost completely replaced all other competitors and all other speech to text systems.

It returns speech-to-text "cooked", with punctuation and disfluences (this is a new word for me) removed. As with all neural network models, it comes in a variety of "sizes" dependent on complexity, which generally correlate to quality of results.


---
class: contentpage
### **Whisper**

A minimum viable command for whisper is 

```sh
whisper media/audio.wav
```

which will use default values for the "model" (neural network discussed before) the language and the output format.




---
class: contentpage
### **Whisper**


The first flag we might want to feed whisper is the language, otherwise it will take a sample of the first 30 seconds and automatically detect.

As an aside, I have noticed that anytime Whisper cannot detect a language if seems to think it is "Welsh".

```sh
whisper --language en media/audio.wav
```



---
class: contentpage
### **Whisper**

output_dir lets us specify an output directory 

```sh
whisper --language en --output_dir whisper_test/ media/audio.wav
```



---
class: contentpage
### **Whisper**


while we are at it, we may also want to pick an output format.

SRTs and VTTs are subtitle tracks with timestamps, which you should be able to drop straight into VLC or DCPs you are creating.

JSON and TSV are useful as structured data, using formats which are common for storing machine-readable data.

We are for now going to use the simplest - a TXT file.

```sh
whisper --language en --output_dir whisper_test/ --output_format txt media/audio.wav
```





---
class: contentpage
### **Whisper**

As mentioned before we can also set the size of the model which is used: base, small, medium, large.

It is worth doing some tests to see what effect the bigger the model has, it will use more disk space and take longer to process, so worth checking the results to see if they are significant enough to justify this.


Matt Miller at Library of Congress has done something similar.
https://thisismattmiller.com/post/lomax-whisper/


```sh
whisper --language en --output_dir whisper_test/ --output_format txt --model large-v2 media/audio.wav
```

PRACTICAL: compare results of base and large-v2 side by side.

Also, there is nothing restricting you to only using the models provided by OpenAI.



---
class: contentpage
### **LLMs**

Now to shift focus to other tools using similar technology, LLMs or Large Language Models.

The most famous of these is undoubtedly ChatGPT from OpenAI, but there are now equivalent models from Meta (Llama), Microsoft (Phi) and Google (Gemma).

We might begin by looking at different things we can do with these models, and then different ways to access them.



---
class: contentpage
### **LLMs**

We are going to dive right in with a multi-modal model called LLaVA, which allows us to do image analysis. Note that we are now about to discover how powerful (or not) our computers are.

Ollama is a cross-platform command line application which us lets run machine learning models very easily from our own machines.

https://ollama.com/

There is a wide selection of models on offer, and as with whisper they range in size, with the expectation that bigger the model better the results (apparently not always true)!

https://ollama.com/library/gemma2

---
class: contentpage
### **LLMs**

Once we have Ollama installed, we can easily call our required model using the following command.

```sh
ollama run llava:7b
```

Bigger models may have systems requirements beyond what our computer can handle, in which case we will get an error message to inform us of this.

---
class: contentpage
### **LLMs**



Back in time if you wanted to do OCR, object detection, or other visual analysis, this would have required dedicated processes. LLMs are a bit different, they are capable of doing it all, we just need to ask the right questions.

---
class: contentpage
### **LLMs**



General description example

```sh
Can you describe image '/home/paulduchesne/Desktop/summer-school-2024/jpg/0087679.jpg'
```



---
class: contentpage
### **LLMs**

OCR example


```sh
What text is in this image '/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg'
```

Please note that accuracy can depend on model size, so this is the one section of the course where having a powerful computer will give you better results, not just faster.
I am currently running the lowest model as my laptop is nothing special, would be keen to compare with people running something better.

For anyone who remembers using Tesseract, mind-bendingly we can ask for specific interpreted text.

```sh
What is the name of the boat in this image '/home/paulduchesne/Desktop/summer-school-2024/jpg/0087024.jpg'
```

---
class: contentpage
### **LLMs**

Object Detection


Frame with boat.

```sh
What objects are visible in '/home/paulduchesne/Desktop/summer-school-2024/jpg/0087024.jpg'
```


Frame with flags.

```sh
What objects are visible in '/home/paulduchesne/Desktop/summer-school-2024/jpg/0087200.jpg'
```

A problem I noticed during testing this was that LLaVA can sometimes be confused when new images are supplied consecutivelu in the same session, so quiting in between may not be a terrible idea.




---
class: contentpage
### **LLMs**


Image characteristics


```sh
Is the following image black and white or colour: '/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg'
```



```sh
What is the aspect ratio of the image area of the following film frame (eg 1.66, 1.85): '/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg'
```

This is especially interesting as the image is letterboxed. There are much easier ways to do "letterbox detection" (e.g. with FFmpeg), but those can be susceptible to black levels.




```sh
What period do you think the following image is from: '/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg'
```


---
class: contentpage
### **LLMs**

So far we have been returning text, which is great, but as we want to integrate with databases and automated systems we want a machine-readable output - LLMs can do this as well.

```sh
Return whether the following image is black and white or colour as a JSON result, following the example {"colour":"Black and White"} or {"colour":"Colour"} and do not return any other text than the JSON: '/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg'
```

---
class: contentpage
### **LLMs**


Now the only piece missing is automating this - copying and pasting lines of text into the terminal is not going to scale if we want to inspect hundreds, thousands or millions of images.

Basic Python example.

```python
import ollama

request = ollama.chat(
    model='llava:7b',
    messages=[
        {
        'role': 'user',
        'content': 'What is the aspect ratio of the image area of the following film frame (eg 1.66, 1.85)',
        'images': ['/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg'],
        }
    ]
)

print(request['message']['content'])
```



---
class: contentpage
### **LLMs**

Convert to a reusable function.

```python
import ollama

def aspect_ratio_detector(frame):
    
    request = ollama.chat(
        model='llava:7b',
        messages=[
            {
            'role': 'user',
            'content': 'What is the aspect ratio of the image area of the following film frame (eg 1.66, 1.85)',
            'images': [frame],
            }
        ]
    )

    return request['message']['content']

result = aspect_ratio_detector('/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg')

print(result)
```





---
class: contentpage
### **LLMs**

Iterate over the full directory of scans.

On my computer this would take a very long time, and there would be little value in processing every image - you would probably want to reduce to one frame per second, or less.

```python

import ollama

def image_summary(frame):
    
    request = ollama.chat(
        model='llava:7b',
        messages=[
            {
            'role': 'user',
            'content': 'Can you describe this image in less than ten words',
            'images': [str(frame)],
            }
        ]
    )

    return request['message']['content']

jpg_path = pathlib.Path.cwd() / 'jpg'
for frame in jpg_path.iterdir():
    result = image_summary(frame)
    print(frame, result)
```








      </textarea>
      <script src="https://remarkjs.com/downloads/remark-latest.min.js" type="text/javascript"></script>
      <script type="text/javascript">var slideshow = remark.create({ratio: "16:9"});</script>
    </body>
  </html>